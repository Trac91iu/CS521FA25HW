{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## CS521 HW1\n",
        "\n",
        "### Problem 1\n",
        "#### 1.1\n",
        "Here, the code snippet is related to calculate the pertubation. Since this is a targeted attack, we need to minimize the loss of the target class t. So here is my answer:"
      ],
      "metadata": {
        "id": "5zaSsXOEdWv4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "adv_x = x - eps * x.grad.sign() # minimize the loss of the target class t"
      ],
      "metadata": {
        "id": "-PLWgFRydZOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.2\n",
        "When I let t=1, the result is 2 with an assertion error.\n",
        "The first thing that I think about is to change the pertubation. I added the epsReal by 0.5, and it was classed to 2. When I added the epsReal by 1, it was classed to 0. So I adjusted epsReal to 0.5+0.7, and it was classed to 1 finally with tensor([[0.0331, 0.0381, 0.0349]], grad_fn=<MmBackward0>). Here, to measure the norm of the difference, I calculated the L2 distance: 3.7947328090667725. And for the targeted class 0 mentioned in 1.1, the L2 distance is 1.5811386108398438.\n",
        "If I made epsReal = 0.5 + 0.64, the L2 distance would be 3.6049959659576416. This is the smallest one that I found. In this case, I believe FGSM attack is ineffective for the given network.\n",
        "\n",
        "<br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>"
      ],
      "metadata": {
        "id": "MaBQCDQHdbpv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Problem 2\n",
        "#### 2.1\n",
        "**Report**\n",
        "\n",
        "pretr_Linf.pth\n",
        "- Standard accuracy 0.85938\n",
        "- Robust accuracy 0.51210 on pgd_linf attack with eps = 0.03137254901960784\n",
        "- Robust accuracy 0.48830 on pgd_l2 attack with eps = 0.75\n",
        "\n",
        "pretr_L2.pth:\n",
        "- Standard accuracy 0.90625\n",
        "- Robust accuracy 0.30850 on pgd_linf attack with eps = 0.03137254901960784\n",
        "- Robust accuracy 0.54380 on pgd_l2 attack with eps = 0.75\n",
        "\n",
        "pretr_RAMP.pth:\n",
        "- Standard accuracy 0.84375\n",
        "- Robust accuracy 0.49760 on pgd_linf attack with eps = 0.03137254901960784\n",
        "- Robust accuracy 0.59650 on pgd_l2 attack with eps = 0.75\n",
        "\n",
        "**Observations**\n",
        "1. Model with weight pretr_L2 is the more accurate one with standard/clean accuracy 0.90625.\n",
        "2. Model with weight pretr_Linf is the most robust one for pgd_linf attack with robust accuracy 0.51210.\n",
        "3. Model with weight pretr_RAMP is the most robust one for pgd_l2 attack with robust accuracy 0.59650.\n",
        "\n",
        "Actually I'm not quite sure about the k, I set it to 10 following the hint. I've attached the code snippets below."
      ],
      "metadata": {
        "id": "PR9eO1VchZsQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. pgd_linf\n",
        "# 1.1 implementation in function pgd_linf_untargeted\n",
        "# TODO: Calculate the loss\n",
        "loss = ce_loss(output, labels)\n",
        "loss.backward()\n",
        "# TODO: compute the adv_x\n",
        "# find delta, clamp with eps\n",
        "delta = eps_step * adv_x.grad.sign() # this is similar to fgsm\n",
        "adv_x = adv_x + delta # gradient ascent to maximize the loss, this is for untargeted attack, so we need to increase the loss\n",
        "new_delta = torch.clamp(adv_x - x, min=-eps, max=eps) # ensure the perturbation is within the eps range\n",
        "adv_x = torch.clamp(x + new_delta, min=0, max=1).detach() # ensure the adv_x image is within the valid range [0, 1]\n",
        "\n",
        "# 1.2 implementation in function test_model_on_single_attack\n",
        "x_adv = pgd_linf_untargeted(model, x_batch, y_batch, k=10, eps=eps, eps_step=eps/4) # model is the preloaded model, x_batch is the input image batch, y_batch is the true label batch, k is set to 10 (actually I'm not sure why 10, maybe just a common choice. I just follow the hint), eps is 0.1, eps_step is eps/4\n",
        "\n",
        "# Then I just run this\n",
        "test_model_on_single_attack(model, attack='pgd_linf')\n",
        "\n",
        "# 2. pgd_l2 implementation in function pgd_l2_untargeted\n",
        "# 2.1 implementation in function pgd_linf_untargeted\n",
        "# TODO: Calculate the loss\n",
        "loss = ce_loss(output, labels)\n",
        "grad = torch.autograd.grad(loss, adv_x, retain_graph=False,create_graph=False)[0] # get the gradient of the loss w.r.t. adv_x, this is different with pgd_linf\n",
        "grad_norms = (torch.norm(grad.view(batch_size, -1), p=2,dim=1) + 1e-10)   # get the l2 norm\n",
        "grad = grad / grad_norms.view(batch_size, 1, 1, 1)\n",
        "# 2.2 compute the adv_x\n",
        "delta = eps_step * grad # this is similar to fgsm, but the grad is normalized to have l2 norm of 1\n",
        "adv_x = adv_x + delta # # this is for untargeted attack, so we need to increase the loss\n",
        "new_delta = adv_x - x\n",
        "new_delta_norms = torch.norm(new_delta.view(batch_size, -1), p=2, dim=1)\n",
        "factor = eps / new_delta_norms # when the norm is greater than eps, then factor is less than 1, else factor is greater than 1\n",
        "factor = torch.min(factor, torch.ones_like(new_delta_norms)) # if the new_delta_norms is greater than eps, then factor is 1, else factor is eps/norm. This make sure factor is at most 1.\n",
        "new_delta = new_delta * factor.view(batch_size, 1, 1, 1) # ensure the perturbation is within the eps range\n",
        "adv_x = torch.clamp(x + new_delta, min=0, max=1).detach() # ensure the adv_x image is within the valid range [0, 1]\n",
        "\n",
        "\n",
        "x_adv = pgd_l2_untargeted(model, x_batch, y_batch, k=10, eps=eps, eps_step=eps/4)"
      ],
      "metadata": {
        "id": "-c3G3apKdgLA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Here is the code for standard evaluation\n",
        "\n",
        "def check_clean(model, test_loader, device):\n",
        "    x_batch, y_batch = next(iter(test_loader))\n",
        "    x_batch = x_batch.to(device)\n",
        "    y_batch = y_batch.to(device)\n",
        "\n",
        "    acc_raw = model(x_batch).argmax(1).eq(y_batch).float().mean().item()\n",
        "\n",
        "    print(f\"Standard accuracy %.5lf\" % (acc_raw))"
      ],
      "metadata": {
        "id": "TDw0blOvsgl3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Here is my evaluation code\n",
        "\n",
        "def eval_model():\n",
        "    # intialize the model\n",
        "    model = PreActResNet18(10, cuda=True, activation='softplus1').to(device)\n",
        "    model.eval()\n",
        "    model.load_state_dict(torch.load('models/pretr_Linf.pth'))\n",
        "    model.eval()\n",
        "    # Evaluate on clean examples\n",
        "    check_clean(model, test_loader, device)\n",
        "    # Evaluate on Linf attack with model 1 with eps = 8/255\n",
        "    test_model_on_single_attack(model, attack='pgd_linf', eps=8/255)\n",
        "    # Evaluate on Linf attack with model 1 with eps = 0.75\n",
        "    test_model_on_single_attack(model, attack='pgd_l2', eps=0.75)\n",
        "\n",
        "    model.load_state_dict(torch.load('models/pretr_L2.pth'))\n",
        "    model.eval()\n",
        "    # Evaluate on clean examples\n",
        "    check_clean(model, test_loader, device)\n",
        "    # Evaluate on Linf attack with model 2 with eps = 8/255\n",
        "    test_model_on_single_attack(model, attack='pgd_linf', eps=8/255)\n",
        "    # Evaluate on Linf attack with model 2 with eps = 0.75\n",
        "    test_model_on_single_attack(model, attack='pgd_l2', eps=0.75)\n",
        "\n",
        "    model.load_state_dict(torch.load('models/pretr_RAMP.pth'))\n",
        "    model.eval()\n",
        "    # Evaluate on clean examples\n",
        "    check_clean(model, test_loader, device)\n",
        "    # Evaluate on Linf attack with model 3 with eps = 8/255\n",
        "    test_model_on_single_attack(model, attack='pgd_linf', eps=8/255)\n",
        "    # Evaluate on Linf attack with model 3 with eps = 0.75\n",
        "    test_model_on_single_attack(model, attack='pgd_l2', eps=0.75)"
      ],
      "metadata": {
        "id": "hNmc8kGKdo7s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br><br><br><br><br><br><br><br><br><br><br><br>\n",
        "\n",
        "#### 2.2\n",
        "\n",
        " **Report**\n",
        "\n",
        " pretr_Linf.pth: Robust accuracy 0.51000 on multi attacks\n",
        "\n",
        " pretr_L2.pth: Robust accuracy 0.30850 on multi attacks\n",
        "\n",
        " pretr_RAMP.pth: Robust accuracy 0.49760 on multi attacks\n",
        "\n",
        " **Observation**\n",
        " 1. Model with weight pretr_Linf is the most robust one against multiple attacks, pretr_RAMP is the second, and pretr_L2 is the last."
      ],
      "metadata": {
        "id": "IDklmYL2dxsg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model_on_multi_attacks(model, eps_linf=8./255., eps_l2=0.75):\n",
        "    model.eval()\n",
        "    tot_test, tot_acc = 0.0, 0.0\n",
        "    for batch_idx, (x_batch, y_batch) in tqdm(enumerate(test_loader), total=len(test_loader), desc=\"Evaluating\"):\n",
        "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "        # TODO: get x_adv_linf and x_adv_l2 untargeted pgd linf and l2 with eps, and eps_step=eps/4\n",
        "        x_adv_linf = pgd_linf_untargeted(model, x_batch, y_batch, k=10, eps=eps_linf, eps_step=eps_linf/4)\n",
        "        x_adv_l2 = pgd_l2_untargeted(model, x_batch, y_batch, k=10, eps=eps_l2, eps_step=eps_l2/4)\n",
        "\n",
        "        ## calculate union accuracy: correct only if both attacks are correct\n",
        "        out = model(x_adv_linf)\n",
        "        pred_linf = torch.max(out, dim=1)[1]\n",
        "        out = model(x_adv_l2)\n",
        "        pred_l2 = torch.max(out, dim=1)[1]\n",
        "\n",
        "        # TODO: get the testing accuracy with multi-norm robustness and update tot_test and tot_acc\n",
        "        tot_acc += ((pred_linf == y_batch) & (pred_l2 == y_batch)).sum().item()\n",
        "        tot_test += y_batch.size(0)\n",
        "\n",
        "    print('Robust accuracy %.5lf' % (tot_acc/tot_test), f'on multi attacks')\n",
        "\n",
        "def eval_model_multi():\n",
        "    # intialize the model\n",
        "    model = PreActResNet18(10, cuda=True, activation='softplus1').to(device)\n",
        "    model.eval()\n",
        "    # Evaluate on L2 attack with different models with eps = 0.5\n",
        "    model.load_state_dict(torch.load('models/pretr_Linf.pth'))\n",
        "    model.eval()\n",
        "    # Evaluate on multi attacks with model 1\n",
        "    test_model_on_multi_attacks(model, eps_linf=8/255, eps_l2=0.5)\n",
        "\n",
        "    model.load_state_dict(torch.load('models/pretr_L2.pth'))\n",
        "    model.eval()\n",
        "    # Evaluate on multi attacks with model 2\n",
        "    test_model_on_multi_attacks(model, eps_linf=8/255, eps_l2=0.5)\n",
        "\n",
        "    model.load_state_dict(torch.load('models/pretr_RAMP.pth'))\n",
        "    model.eval()\n",
        "    # Evaluate on multi attacks with model 3\n",
        "    test_model_on_multi_attacks(model, eps_linf=8/255, eps_l2=0.5)"
      ],
      "metadata": {
        "id": "45AWKQv3vUjo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br><br><br><br><br>\n",
        "### Problem 3\n",
        "\n",
        "#### Summary\n",
        "The paper is mainly about building a benchmark for evaluating 19 types of unforseen adversaries.\n",
        "\n",
        "#### Strengths\n",
        "1. The topic is interesting. Unforseen robustness means the robustness for models that are not trained on specific type of attacks. For automotives, there can easily be adversary attacks (e.g., stop signs), and it is not possible to collect all of the attacks and train a model to against all of them. The research question is valuable and important.\n",
        "2. The paper proposes a new metric called UA2 to measure the unforeseen robustness. The paper does provide more evaluation against more attacks, which makes UA2 more useful to assess the performance under more adversarial settings.\n",
        "3. The paper explores methods to improve unforseen robustness, and discusses how training strategies affect unforeseen robustness. These provide insights for future works.\n",
        "#### Weaknesses\n",
        "1. The paper evaluates on 19 non-Lp attacks, but it is unclear how they are systematically collected, and whether they are representative of real-world scenarios. In subsection 4.2, for the 8 core attacks, although most are new, but it's not clear how they are likely to occur in real world settings. This is important because some models may perform well on some specific scenarios, there may be attacks other than the given ones, how to measure the completeness of the attacks?\n",
        "2. For the evaluation, in subsection 5.1, the paper mentions that UA2 after optimisation is similar to worst case robustness, and similar to L_p robust accuracy, why it is a new metric? Furthermore, I'm curious about what optimisation makes UA2 similar to worst case robustness? It seems the authors didn't provide any explaination.Besides, the paper mentions that \"We conclude that UA2 is distinct from L_p robustness, and present UA2 as an improved progress measure when working towards real-world worst-case robustness.\" However, is it really an improved progress measure, or just behaves different for specific scenarios?\n",
        "3. The paper doesn't seem self-contained. I find it relies heavily on the appendix. For example, in section 5, the paper simply discussed the differences with other works like ImageNet-C and other works, but the quantitative results are all in the appendix. But the differences with previous works are very important, right? Besides, the paper has done a lot of measurement works, but\n",
        "\n",
        "#### Extensions\n",
        "I'm curious about how the proposed approach and metric could be applied to real-world scenarios, such as automotives, robotics, and other safety-related domains. These domains involves richer contexts and complex sources of unforeseen adversaries, which may have overlap with the ones discussed in the paper. Exploring UA2 in such scenarios help validate whether it indeed is a useful metric, or whether it needs further extend to domain-specific constraints. This would enlarge the real-world impact of this work."
      ],
      "metadata": {
        "id": "tuURQAFzwlrU"
      }
    }
  ]
}